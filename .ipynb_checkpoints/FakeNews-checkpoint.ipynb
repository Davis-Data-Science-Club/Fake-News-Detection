{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "97896fdd-82e6-4469-b0e0-5861b3b4d30c",
   "metadata": {},
   "source": [
    "# Setting up virtual environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5445a5cc-a1a3-454b-b8f7-4d0012eff62e",
   "metadata": {},
   "source": [
    "Install Mambaforge https://mamba.readthedocs.io/en/latest/installation.html\n",
    "If you're on MacOS you can run `brew install mambaforge`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c7debb-f773-49b7-a332-a76248db8a03",
   "metadata": {},
   "source": [
    "Create a virtual environment for the project\n",
    "`mamba create -n fakenews python=3.10`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4287f82d-4a7a-460c-935e-13a1311bf4d0",
   "metadata": {},
   "source": [
    "Activate the env `mamba activate fakenews`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db41f471-d982-42a7-be5e-90c0db011000",
   "metadata": {},
   "source": [
    "Install dependencies we'll need for the project\n",
    "`mamba install -c huggingface transformers=4.26.0 datasets evaluate jupyterlab scikit-learn`\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f81626d-7b99-486a-b1c6-e36503c89e3b",
   "metadata": {},
   "source": [
    "Run jupyterlab:\n",
    "`jupyter lab`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "da8b7ce3-816e-4b15-92ba-0fdaf571a885",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): ...working... done\n",
      "Solving environment: ...working... done\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: C:\\Users\\Erick\\anaconda3\\envs\\fakenews\n",
      "\n",
      "  added / updated specs:\n",
      "    - pytorch\n",
      "\n",
      "\n",
      "The following packages will be downloaded:\n",
      "\n",
      "    package                    |            build\n",
      "    ---------------------------|-----------------\n",
      "    future-0.18.2              |  py310haa95532_1         672 KB\n",
      "    libuv-1.40.0               |       he774522_0         255 KB\n",
      "    ninja-1.10.2               |       haa95532_5          14 KB\n",
      "    ninja-base-1.10.2          |       h6d14046_5         255 KB\n",
      "    pytorch-1.12.1             |cpu_py310h5e1f01c_0        80.7 MB\n",
      "    ------------------------------------------------------------\n",
      "                                           Total:        81.9 MB\n",
      "\n",
      "The following NEW packages will be INSTALLED:\n",
      "\n",
      "  future             pkgs/main/win-64::future-0.18.2-py310haa95532_1\n",
      "  libuv              pkgs/main/win-64::libuv-1.40.0-he774522_0\n",
      "  ninja              pkgs/main/win-64::ninja-1.10.2-haa95532_5\n",
      "  ninja-base         pkgs/main/win-64::ninja-base-1.10.2-h6d14046_5\n",
      "  pytorch            pkgs/main/win-64::pytorch-1.12.1-cpu_py310h5e1f01c_0\n",
      "\n",
      "\n",
      "\n",
      "Downloading and Extracting Packages\n",
      "\n",
      "pytorch-1.12.1       | 80.7 MB   |            |   0% \n",
      "pytorch-1.12.1       | 80.7 MB   |            |   0% \n",
      "pytorch-1.12.1       | 80.7 MB   |            |   0% \n",
      "pytorch-1.12.1       | 80.7 MB   |            |   1% \n",
      "pytorch-1.12.1       | 80.7 MB   | 1          |   2% \n",
      "pytorch-1.12.1       | 80.7 MB   | 2          |   3% \n",
      "pytorch-1.12.1       | 80.7 MB   | 4          |   5% \n",
      "pytorch-1.12.1       | 80.7 MB   | 6          |   6% \n",
      "pytorch-1.12.1       | 80.7 MB   | 8          |   8% \n",
      "pytorch-1.12.1       | 80.7 MB   | 9          |  10% \n",
      "pytorch-1.12.1       | 80.7 MB   | #1         |  12% \n",
      "pytorch-1.12.1       | 80.7 MB   | #3         |  14% \n",
      "pytorch-1.12.1       | 80.7 MB   | #5         |  16% \n",
      "pytorch-1.12.1       | 80.7 MB   | #7         |  18% \n",
      "pytorch-1.12.1       | 80.7 MB   | #9         |  20% \n",
      "pytorch-1.12.1       | 80.7 MB   | ##1        |  22% \n",
      "pytorch-1.12.1       | 80.7 MB   | ##3        |  23% \n",
      "pytorch-1.12.1       | 80.7 MB   | ##4        |  24% \n",
      "pytorch-1.12.1       | 80.7 MB   | ##5        |  25% \n",
      "pytorch-1.12.1       | 80.7 MB   | ##6        |  26% \n",
      "pytorch-1.12.1       | 80.7 MB   | ##7        |  28% \n",
      "pytorch-1.12.1       | 80.7 MB   | ##9        |  29% \n",
      "pytorch-1.12.1       | 80.7 MB   | ###        |  30% \n",
      "pytorch-1.12.1       | 80.7 MB   | ###2       |  32% \n",
      "pytorch-1.12.1       | 80.7 MB   | ###4       |  34% \n",
      "pytorch-1.12.1       | 80.7 MB   | ###6       |  36% \n",
      "pytorch-1.12.1       | 80.7 MB   | ###8       |  38% \n",
      "pytorch-1.12.1       | 80.7 MB   | ####       |  40% \n",
      "pytorch-1.12.1       | 80.7 MB   | ####1      |  42% \n",
      "pytorch-1.12.1       | 80.7 MB   | ####3      |  43% \n",
      "pytorch-1.12.1       | 80.7 MB   | ####4      |  44% \n",
      "pytorch-1.12.1       | 80.7 MB   | ####5      |  46% \n",
      "pytorch-1.12.1       | 80.7 MB   | ####6      |  47% \n",
      "pytorch-1.12.1       | 80.7 MB   | ####7      |  48% \n",
      "pytorch-1.12.1       | 80.7 MB   | ####9      |  49% \n",
      "pytorch-1.12.1       | 80.7 MB   | #####1     |  51% \n",
      "pytorch-1.12.1       | 80.7 MB   | #####3     |  53% \n",
      "pytorch-1.12.1       | 80.7 MB   | #####4     |  55% \n",
      "pytorch-1.12.1       | 80.7 MB   | #####6     |  57% \n",
      "pytorch-1.12.1       | 80.7 MB   | #####8     |  59% \n",
      "pytorch-1.12.1       | 80.7 MB   | ######     |  61% \n",
      "pytorch-1.12.1       | 80.7 MB   | ######2    |  62% \n",
      "pytorch-1.12.1       | 80.7 MB   | ######4    |  65% \n",
      "pytorch-1.12.1       | 80.7 MB   | ######6    |  66% \n",
      "pytorch-1.12.1       | 80.7 MB   | ######8    |  68% \n",
      "pytorch-1.12.1       | 80.7 MB   | #######    |  70% \n",
      "pytorch-1.12.1       | 80.7 MB   | #######2   |  73% \n",
      "pytorch-1.12.1       | 80.7 MB   | #######4   |  75% \n",
      "pytorch-1.12.1       | 80.7 MB   | #######6   |  77% \n",
      "pytorch-1.12.1       | 80.7 MB   | #######8   |  79% \n",
      "pytorch-1.12.1       | 80.7 MB   | ########   |  81% \n",
      "pytorch-1.12.1       | 80.7 MB   | ########2  |  83% \n",
      "pytorch-1.12.1       | 80.7 MB   | ########5  |  85% \n",
      "pytorch-1.12.1       | 80.7 MB   | ########7  |  87% \n",
      "pytorch-1.12.1       | 80.7 MB   | ########9  |  89% \n",
      "pytorch-1.12.1       | 80.7 MB   | #########1 |  91% \n",
      "pytorch-1.12.1       | 80.7 MB   | #########3 |  93% \n",
      "pytorch-1.12.1       | 80.7 MB   | #########5 |  95% \n",
      "pytorch-1.12.1       | 80.7 MB   | #########7 |  97% \n",
      "pytorch-1.12.1       | 80.7 MB   | #########9 |  99% \n",
      "pytorch-1.12.1       | 80.7 MB   | ########## | 100% \n",
      "\n",
      "ninja-base-1.10.2    | 255 KB    |            |   0% \n",
      "ninja-base-1.10.2    | 255 KB    | ########## | 100% \n",
      "ninja-base-1.10.2    | 255 KB    | ########## | 100% \n",
      "\n",
      "future-0.18.2        | 672 KB    |            |   0% \n",
      "future-0.18.2        | 672 KB    | ########## | 100% \n",
      "future-0.18.2        | 672 KB    | ########## | 100% \n",
      "\n",
      "ninja-1.10.2         | 14 KB     |            |   0% \n",
      "ninja-1.10.2         | 14 KB     | ########## | 100% \n",
      "\n",
      "libuv-1.40.0         | 255 KB    |            |   0% \n",
      "libuv-1.40.0         | 255 KB    | ########## | 100% \n",
      "libuv-1.40.0         | 255 KB    | ########## | 100% \n",
      "Preparing transaction: ...working... done\n",
      "Verifying transaction: ...working... done\n",
      "Executing transaction: ...working... done\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "==> WARNING: A newer version of conda exists. <==\n",
      "  current version: 4.12.0\n",
      "  latest version: 23.1.0\n",
      "\n",
      "Please update conda by running\n",
      "\n",
      "    $ conda update -n base -c defaults conda\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "conda install pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5e825345-22e0-4716-8d95-a7759aca619b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mbackends\u001b[38;5;241m.\u001b[39mmps\u001b[38;5;241m.\u001b[39mis_available():\n\u001b[0;32m      3\u001b[0m     mps_device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmps\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "if torch.backends.mps.is_available():\n",
    "    mps_device = torch.device(\"mps\")\n",
    "    x = torch.ones(1, device=mps_device)\n",
    "    print (x)\n",
    "else:\n",
    "    print (\"MPS device not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "95e05d36-a44a-493a-bc53-4659b446d63b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Erick\\anaconda3\\envs\\fakenews\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, load_from_disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2ce7960c-7be8-4dbc-a376-e28df6a5a5f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset liar (C:/Users/Erick/.cache/huggingface/datasets/liar/default/1.0.0/479463e757b7991eed50ffa7504d7788d6218631a484442e2098dabbf3b44514)\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 251.43it/s]\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"liar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4fca8042-0852-4e1b-8628-d268babfc48d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-16166e5dc31fa63c\n",
      "Found cached dataset csv (C:/Users/Erick/.cache/huggingface/datasets/csv/default-16166e5dc31fa63c/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317)\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 125.26it/s]\n"
     ]
    }
   ],
   "source": [
    "fake_csv = load_dataset(\"csv\", data_files=\"news.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "37ac3dec-55d3-4002-a269-49345e2a7e69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['Unnamed: 0', 'title', 'text', 'label'],\n",
       "        num_rows: 6335\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fake_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9efbad4f-e784-4996-abc7-5621c43294b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_csv_split = fake_csv[\"train\"].train_test_split(test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a3ec57da-d63c-4564-82e7-d385a6a3cf47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['Unnamed: 0', 'title', 'text', 'label'],\n",
       "        num_rows: 5701\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['Unnamed: 0', 'title', 'text', 'label'],\n",
       "        num_rows: 634\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fake_csv_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e9617324-3499-4d6a-9919-00260c78132d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load DistilBERT tokenizer and tokenize (encode) the texts\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "13c914c4-1f3a-46cc-8acf-462cc9bc08f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 5701/5701 [00:01<00:00, 5433.81ex/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 634/634 [00:00<00:00, 6832.71ex/s]\n"
     ]
    }
   ],
   "source": [
    "fake_csv_split = fake_csv_split.map(lambda x: {\"label\": 1 if x[\"label\"] == \"FAKE\" else 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2a8a2045-3739-4adf-9a4c-a7af7d6ef3d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "def tokenize(batch):\n",
    "    from transformers import AutoTokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "    tokenized_batch = tokenizer(batch['text'], padding=True, truncation=True, max_length=128)\n",
    "    return tokenized_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ca97a6f9-11ca-4c3e-b101-7f2c64d9fdf5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['label', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 5701\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['label', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 634\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fake_csv_split.map(tokenize, remove_columns=['Unnamed: 0', 'title', 'text'], batched=True, num_proc=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "652820ed-8fa6-4f3a-9163-8f0002880eb3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at C:\\Users\\Erick\\.cache\\huggingface\\datasets\\csv\\default-16166e5dc31fa63c\\0.0.0\\6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317\\cache-50286481a9f1f581.arrow\n",
      "Loading cached processed dataset at C:\\Users\\Erick\\.cache\\huggingface\\datasets\\csv\\default-16166e5dc31fa63c\\0.0.0\\6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317\\cache-b1870c906afc97a0.arrow\n",
      "Loading cached processed dataset at C:\\Users\\Erick\\.cache\\huggingface\\datasets\\csv\\default-16166e5dc31fa63c\\0.0.0\\6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317\\cache-a4dc9cc34e37af29.arrow\n",
      "Loading cached processed dataset at C:\\Users\\Erick\\.cache\\huggingface\\datasets\\csv\\default-16166e5dc31fa63c\\0.0.0\\6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317\\cache-0e9cd019af4684c8.arrow\n",
      "Loading cached processed dataset at C:\\Users\\Erick\\.cache\\huggingface\\datasets\\csv\\default-16166e5dc31fa63c\\0.0.0\\6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317\\cache-c629d449fbd1d967.arrow\n",
      "Loading cached processed dataset at C:\\Users\\Erick\\.cache\\huggingface\\datasets\\csv\\default-16166e5dc31fa63c\\0.0.0\\6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317\\cache-8668fb5d1cc561c9.arrow\n",
      "Loading cached processed dataset at C:\\Users\\Erick\\.cache\\huggingface\\datasets\\csv\\default-16166e5dc31fa63c\\0.0.0\\6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317\\cache-194968e0a899c90e.arrow\n",
      "Loading cached processed dataset at C:\\Users\\Erick\\.cache\\huggingface\\datasets\\csv\\default-16166e5dc31fa63c\\0.0.0\\6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317\\cache-b159a183d0a0102d.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['label', 'input_ids', 'attention_mask']\n"
     ]
    }
   ],
   "source": [
    "# Tokenize and encode the dataset\n",
    "\n",
    "\n",
    "dataset_enc = fake_csv_split.map(tokenize, remove_columns=['Unnamed: 0', 'title', 'text'], batched=True, num_proc=4)\n",
    "\n",
    "# Set dataset format for PyTorch\n",
    "dataset_enc.set_format('torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "\n",
    "# Check the output\n",
    "print(dataset_enc[\"train\"].column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "60b4923d-b09c-4e47-b96d-26fc469f0992",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Instantiate a data collator with dynamic padding\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0b8ed82c-301b-4247-bb71-c657808952b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)\"pytorch_model.bin\";: 100%|██████████████████████████████████████████| 268M/268M [00:16<00:00, 16.0MB/s]\n",
      "C:\\Users\\Erick\\anaconda3\\envs\\fakenews\\lib\\site-packages\\huggingface_hub-0.12.0-py3.8.egg\\huggingface_hub\\file_download.py:129: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Erick\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias', 'vocab_transform.weight', 'vocab_layer_norm.weight']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'pre_classifier.bias', 'classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "# Load model from checkpoint\\n\",\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\",\n",
    "                                                           num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f9f9884d-77da-4b23-8c79-eb6820adc030",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "PyTorch is not linked with support for mps devices",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\fakenews\\lib\\site-packages\\transformers\\modeling_utils.py:1749\u001b[0m, in \u001b[0;36mPreTrainedModel.to\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1744\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`.to` is not supported for `8-bit` models. Please use the model as it is, since the\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m model has already been set to the correct devices and casted to the correct `dtype`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1747\u001b[0m     )\n\u001b[0;32m   1748\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\fakenews\\lib\\site-packages\\torch\\nn\\modules\\module.py:927\u001b[0m, in \u001b[0;36mModule.to\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    923\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    924\u001b[0m                     non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[0;32m    925\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, non_blocking)\n\u001b[1;32m--> 927\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\fakenews\\lib\\site-packages\\torch\\nn\\modules\\module.py:579\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    577\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn):\n\u001b[0;32m    578\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[1;32m--> 579\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    581\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    582\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    583\u001b[0m             \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    584\u001b[0m             \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    589\u001b[0m             \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    590\u001b[0m             \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\fakenews\\lib\\site-packages\\torch\\nn\\modules\\module.py:579\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    577\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn):\n\u001b[0;32m    578\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[1;32m--> 579\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    581\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    582\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    583\u001b[0m             \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    584\u001b[0m             \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    589\u001b[0m             \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    590\u001b[0m             \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\fakenews\\lib\\site-packages\\torch\\nn\\modules\\module.py:579\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    577\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn):\n\u001b[0;32m    578\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[1;32m--> 579\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    581\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    582\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    583\u001b[0m             \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    584\u001b[0m             \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    589\u001b[0m             \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    590\u001b[0m             \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\fakenews\\lib\\site-packages\\torch\\nn\\modules\\module.py:602\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    598\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[0;32m    599\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[0;32m    600\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[0;32m    601\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m--> 602\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    603\u001b[0m should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[0;32m    604\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\fakenews\\lib\\site-packages\\torch\\nn\\modules\\module.py:925\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[1;34m(t)\u001b[0m\n\u001b[0;32m    922\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[0;32m    923\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    924\u001b[0m                 non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[1;32m--> 925\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: PyTorch is not linked with support for mps devices"
     ]
    }
   ],
   "source": [
    "model.to(\"mps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "28e624f1-c326-4c1e-9d07-cc2364737595",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "training_args = TrainingArguments(output_dir=\"test_trainer\",  evaluation_strategy=\"epoch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1b623bd3-6deb-4980-aa13-1551c35e9c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"accuracy\")\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "343f7b57-a1b1-46ce-ac5f-433494d8a3cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset_enc[\"train\"],\n",
    "    eval_dataset=dataset_enc[\"test\"],\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "400a06b1-0c93-48b2-b61f-0d30c9d266da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import lxml.html as lx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd217e8d-0dee-43cb-9ab4-f41fe4a4d0fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://www.foxnews.com/politics/ahead-biden-state-union-address-country-dissatisfied-multiple-crises\"\n",
    "response = requests.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6601b329-971c-43d9-af97-2c67ff594d12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Response.raise_for_status of <Response [200]>>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.raise_for_status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1ee211fb-39ac-443b-ab3c-36342e8adfa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5206e227-10e6-4220-b654-fd4ecb3b74fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Ahead of Biden State of the Union address, country dissatisfied with state of the union after multiple crises'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "headline = soup.find_all(class_ = \"headline\")[0]\n",
    "headline = str(headline)\n",
    "headline_clean = headline.replace('<h1 class=\"headline\">',\"\").replace(\"</h1>\",\"\")\n",
    "headline_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "1abbcacf-9eae-4ecc-bf56-3bc163daad92",
   "metadata": {},
   "outputs": [],
   "source": [
    "text2 = soup.find_all(class_= \"article-body\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "2d0cdb58-10ab-4549-9622-23a29112a50c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Element html at 0x1e975523680>"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "html = lx.fromstring(response.text)\n",
    "html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "139944a5-3010-4cfa-8ff9-5fb8a442c30a",
   "metadata": {},
   "outputs": [],
   "source": [
    "string = html.xpath('//p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "a11dcdc5-1324-4b48-ad62-abed4cbd88a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(string)\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "347fde32-9ceb-4252-9057-1b2d729f2248",
   "metadata": {},
   "outputs": [],
   "source": [
    "lst  = []   \n",
    "for i in range(0, len(string)):\n",
    "    article_part = html.xpath('//p')[i].text_content()\n",
    "    article_part = str(article_part)\n",
    "    article_part = article_part.replace(\"\\n \",\"\").replace(\"  \", \"\").replace(\"\\'\" , \" \").replace(\"\\xa0\",\"\")\n",
    "    lst.append(article_part)           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "9f16c177-763e-4024-bfcb-5469ac46a13c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "58bb6387-4fcd-44ef-bdb1-57400b48ef7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' This material may not be published, broadcast, rewritten, or redistributed. ©2023 FOX News Network, LLC. All rights reserved. Quotes displayed in real-time or delayed by at least 15 minutes. Market data provided by Factset. Powered and implemented by FactSet Digital Solutions. Legal Statement. Mutual Fund and ETF data provided by Refinitiv Lipper. Former House Speaker Newt Gingrich joins  Fox & Friends  to discuss President Biden s political future and performance in office ahead of his State of the Union Address.President Biden will have a tough audience among the American people on Tuesday night when he delivers the annual State of the Union address.Speaking from the Capitol, Biden is expected to make the case to the American people in his second State of the Union speech that the country is in a strong and prosperous position now and moving forward.New polling finds that high percentages of the American people feel financially worse off since Biden took office and are unhappy with the direction of the country, highlighting a general pessimism among the population that s been in place for months.President Joe Biden pauses as he listens to a question during a news conference at the White House, March 25, 2021. (AP Photo/Evan Vucci) FOX NEWS POLL: STATE OF THE UNION IS DYSFUNCTION, DISSATISFACTION AND DISAPPROVALOnly 4 in 10 Americans say the state of the union is strong, according to a new Monmouth University poll out Monday. Just 7% call it \"very strong\" while 32% say the country is in a \"somewhat strong\" state. Meanwhile, 32% deem the union \"not too strong\" and 26% say it s \"not at all strong.\"The number of Americans who feel the state of the union is at least somewhat strong has steadily declined over the past five years, from 55% in 2018 to 39% in the latest survey.The Monmouth poll is hardly alone. According to a recent Marist poll, more than 60% of people think the state of the union isn t strong — including 72% of independents.To make matters worse for Biden s speech, Monmouth also found that an overwhelming majority of Americans, 73%, think the country is on the wrong track, compared to just 24% who feel the U.S. is headed in the right direction.A recent NBC News poll similarly found that 71% of Americans say the country is headed in the wrong direction. Dating back to September, NBC News surveys have repeatedly shown each month that no less than 68% of people think the country is on the wrong track — an unprecedented level of sustained pessimism in the poll s 30-year history.73% of voters disapprove of how Democrats are handling inflation. (istock)AS BIDEN TOUTS US ECONOMY, AMERICANS STRUGGLING TO MAKE CAR PAYMENTSLast year, polling consistently found that more than 70% or even 80% of the country saw the U.S. as being on the wrong track and as little as 10% expressing satisfaction with its direction.To add to Biden s uphill climb Tuesday night, a striking 4 in 10 Americans say they ve gotten worse off financially since Biden became president, according to a new ABC News/Washington Post poll. That s the highest amount in the poll s 37-year history.Several polls over the last year have similarly shown a strong plurality or even a majority of Americans saying they re financially worse off under Biden.A likely cause of such economic distress is inflation, which continues to eat away at household income and drive persistent public disapproval in Biden s handling of the economy.President Joe Biden takes part in a virtual meeting during his trip to Jerusalem, on July 14, 2022. (Mandel Ngan/AFP via Getty Images)BIDEN BATTERED BY CRISES SINCE LAST STATE OF THE UNIONThis wave of unfavorable polling comes as Biden s already being battered by a series of domestic and foreign policy crises since declaring the state of the union to be strong during last year s address.Nonetheless, Tuesday night will offer Biden an opportunity to persuade the American people that their pessimism is misplaced. The State of the Union address will also allow the president to make an early pitch for re-election, should he decide to run again in 2024.However, that too may be a tough pitch — even among his own base.Indeed, a striking 62% of Democrats don t want Biden to run for re-election, compared to just 37% who want to see him seek a second term, according to a new Associated Press/NORC Center for Public Affairs Research poll.CLICK HERE TO GET THE FOX NEWS APPThe new survey came one day after the ABC News/Washington Post poll found that by a 58-31% margin, Democrats want their party to nominate someone other than Biden to be their 2024 presidential nominee.The White House did not immediately respond to Fox News Digital s request for comment.Aaron Kliegman is a politics reporter for Fox News Digital.Get the latest updates from the 2024 campaign trail, exclusive interviews and more Fox News politics content.SubscribedYou ve successfully subscribed to this newsletter! This material may not be published, broadcast, rewritten, or redistributed. ©2023 FOX News Network, LLC. All rights reserved. Quotes displayed in real-time or delayed by at least 15 minutes. Market data provided by Factset. Powered and implemented by FactSet Digital Solutions. Legal Statement. Mutual Fund and ETF data provided by Refinitiv Lipper. '"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "article_full = \"\".join(lst)\n",
    "article_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "c0e65667-e7a3-4d70-b34b-bee2c99ee1bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Response.raise_for_status of <Response [403]>>"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = \"https://link.h-cdn.com/get\"\n",
    "header = {'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/110.0.0.0 Safari/537.36 Edg/110.0.1587.41'}\n",
    "response = requests.get(url)\n",
    "response.raise_for_status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "78ad59d4-cb9b-412a-be69-7ff0954bbe27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Response.raise_for_status of <Response [404]>>"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = requests.get(\"https://link.h-cdn.com/get\", params = {\n",
    "    \"Host\": \"link.h-cdn.com\",\n",
    "    \"Origin\" : \"https://www.foxnews.com\",\n",
    "    })\n",
    "\n",
    "response.raise_for_status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "59c2c061-12d4-4ce5-80a9-da60a020c021",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Response.raise_for_status of <Response [200]>>"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = requests.get(\"https://www.foxnews.com/politics\")\n",
    "response.raise_for_status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "87480ebe-4dbd-4cc9-9d5c-c5f3d8e949aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "16bfda74-5e45-440a-90e7-e184b56cb8ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "html = lx.fromstring(response.text)\n",
    "lst_links = html.xpath('//h4/a/@href')\n",
    "for i in range(0,len(lst_links)):\n",
    "    if '/politics/' not in  lst_links[i]:\n",
    "        lst_links[i] = ''\n",
    "    else:\n",
    "        continue\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "8b77ee4b-ca18-407b-a0b3-12ef6de144e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/politics/faa-closes-airspace-montana-support-department-defense-activities',\n",
       " '/politics/dems-cite-ginni-thomas-mitch-mcconnell-reasons-impose-code-conduct-supreme-court-justices',\n",
       " '/politics/alaska-sen-murkowski-us-must-send-message-we-dont-tolerate-violation-us-airspace',\n",
       " '/politics/washington-post-corrects-comical-story-gop-rep-anna-paulina-luna-second-time',\n",
       " '/politics/johnson-warns-left-infiltrated-major-us-institutions-gop-warns-weaponization-of-government',\n",
       " '/politics/pentagon-says-us-detected-third-flying-object-alaska-day-shooting-canada',\n",
       " '/politics/republicans-react-third-object-being-shot-canada-unprecedented-challenge',\n",
       " '/politics/dems-cite-ginni-thomas-mitch-mcconnell-reasons-impose-code-conduct-supreme-court-justices',\n",
       " '/politics/china-mocks-biden-knocking-down-balloons-fighter-jets-hysterical-laughably-juvenile',\n",
       " '/politics/border-patrol-nabs-illegal-immigrants-smuggler-spotting-suv-driving-erratically-shredded-tire',\n",
       " '/politics/money-talks-rumored-presidential-hopefuls-most-cash-bank',\n",
       " '/politics/house-gop-warns-bidens-blunders-chinese-spy-crafts-could-impact-alarming-military-recruitment-crisis',\n",
       " '/politics/white-house-climate-czar-met-privately-eco-group-pushing-gas-stove-bans',\n",
       " '/politics/biden-admin-cracks-down-washers-fridges-latest-climate-action-overregulation-steroids']"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for j in lst_links[:]:\n",
    "    if j == '':\n",
    "        lst_links.remove(j)\n",
    "lst_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1353d54e-1882-4eba-99de-a128dc6780b5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
